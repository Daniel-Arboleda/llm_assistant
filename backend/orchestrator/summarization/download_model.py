import os
import torch
from transformers import AutoModelForCausalLM, AutoTokenizer

# ğŸ“Œ Verificar disponibilidad de GPU
print("ğŸš€ Verificando disponibilidad de GPU...")
gpu_available = torch.cuda.is_available()

# ğŸ“Œ ConfiguraciÃ³n de memoria (ajustable)
GPU_MEMORY_LIMIT_GB = 3  # ğŸ”§ Ajusta el lÃ­mite de memoria de GPU
TOTAL_GPU_MEMORY_GB = torch.cuda.get_device_properties(0).total_memory / 1e9 if gpu_available else 0

if gpu_available:
    print(f"âœ… GPU detectada: {torch.cuda.get_device_name(0)} con {TOTAL_GPU_MEMORY_GB:.2f} GB")

    # ğŸ“Œ Establecer lÃ­mite de memoria de GPU si es posible
    memory_fraction = min(GPU_MEMORY_LIMIT_GB / TOTAL_GPU_MEMORY_GB, 1)
    torch.cuda.set_per_process_memory_fraction(memory_fraction, 0)

    # ğŸ“Œ Evitar fragmentaciÃ³n de memoria en CUDA
    os.environ["PYTORCH_CUDA_ALLOC_CONF"] = "expandable_segments:True"

else:
    print("âš ï¸ No se detectÃ³ GPU. El modelo se ejecutarÃ¡ en CPU.")

# ğŸ“Œ Ruta donde se guardarÃ¡ el modelo
model_path = "./model"

# ğŸ“Œ Verificar si el modelo ya estÃ¡ descargado
if not os.path.exists(model_path) or not os.listdir(model_path):
    print("ğŸ”½ Descargando el modelo GPT-Neo 1.3B...")

    try:
        # ğŸ“Œ Cargar modelo optimizando el uso mixto de GPU y RAM
        model = AutoModelForCausalLM.from_pretrained(
            "EleutherAI/gpt-neo-1.3B",
            torch_dtype=torch.float16 if gpu_available else torch.float32,  # Reducir consumo de memoria
            device_map="auto" if gpu_available else None,  # AsignaciÃ³n automÃ¡tica de GPU y RAM
            offload_folder="./offload" if gpu_available else None,  # Mover partes del modelo a RAM si es necesario
            low_cpu_mem_usage=True
        )
        tokenizer = AutoTokenizer.from_pretrained("EleutherAI/gpt-neo-1.3B")

        # ğŸ“Œ Guardar el modelo en la carpeta local
        model.save_pretrained(model_path)
        tokenizer.save_pretrained(model_path)
        print("âœ… Modelo descargado y guardado en:", model_path)

    except ImportError:
        print("âŒ Error: Falta un paquete requerido. Ejecuta:")
        print("    pip install accelerate")
    except Exception as e:
        print(f"âŒ Error inesperado: {e}")

else:
    print("âœ… El modelo ya estÃ¡ descargado. No se necesita descargar nuevamente.")
